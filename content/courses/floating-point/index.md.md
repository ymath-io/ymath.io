Floating-point numbers are the most common way of representing real numbers in computing. As such, they are an essential part of most modern programs and underpin much of the world's most critical infrastructure. They address a fundamental problem of computer science: Computers are discrete, numbers are not. Understanding floating-point arithmetic---and its limitations---is essential for numerical analysis and helpful for any serious programmer.

Computers directly represent integers in binary. The integer $14$, for example, is stored as $1110_2$,


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTM5Njk3NDk1Nl19
-->